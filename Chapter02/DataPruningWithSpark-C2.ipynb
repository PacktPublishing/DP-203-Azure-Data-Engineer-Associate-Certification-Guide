{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c6f10c98-0681-479a-9297-63977f20f894","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"de7ed935-dcba-42f0-a9e7-89f33dc9a494","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e607f0b9-89b2-4f2b-9912-2e12f83dc56e","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import *\n","\n","columnNames = [\"tripId\",\"driverId\",\"customerId\",\"cabId\",\"tripDate\",\"startLocation\",\"endLocation\"]\n","tripData = [\n","  ('100', '200', '300', '400', '20220101', 'New York', 'New Jersey'),\n","  ('101', '201', '301', '401', '20220102', 'Tempe', 'Phoenix'),\n","  ('102', '202', '302', '402', '20220103', 'San Jose', 'San Franciso'),\n","  ('103', '203', '303', '403', '20220102', 'New York', 'Boston'),\n","  ('104', '204', '304', '404', '20220103', 'New York', 'Washington'),\n","  ('105', '205', '305', '405', '20220201', 'Miami', 'Fort Lauderdale'),\n","  ('106', '206', '306', '406', '20220202', 'Seattle', 'Redmond'),\n","  ('107', '207', '307', '407', '20220203', 'Los Angeles', 'San Diego'),\n","  ('108', '208', '308', '408', '20220301', 'Phoenix', 'Las Vegas'),\n","  ('109', '209', '309', '409', '20220302', 'Washington', 'Baltimore'),\n","  ('110', '210', '310', '410', '20220303', 'Dallas', 'Austin'),\n","  ('111', '211', '311', '411', '20220303', 'New York', 'New Jersey'),\n","  ('112', '212', '312', '412', '20220304', 'New York', 'Boston'),\n","  ('113', '212', '312', '412', '20220401', 'San Jose', 'San Ramon'),\n","  ('114', '212', '312', '412', '20220404', 'San Jose', 'Oakland'),\n","  ('115', '212', '312', '412', '20220404', 'Tempe', 'Scottsdale'),\n","  ('116', '212', '312', '412', '20220405', 'Washington', 'Atlanta'),\n","  ('117', '212', '312', '412', '20220405', 'Seattle', 'Portland'),\n","  ('118', '212', '312', '412', '20220405', 'Miami', 'Tampa')\n","]\n","df = spark.createDataFrame(data= tripData, schema = columnNames)\n","\n","# Split the data according the current timestamp and write to store as parquet files\n","dftripDate = df.withColumn(\"tripDate\", to_timestamp(col(\"tripDate\"), 'yyyyMMdd')) \\\n","           .withColumn(\"year\", tripDate_format(col(\"tripDate\"), \"yyyy\")) \\\n","           .withColumn(\"month\", tripDate_format(col(\"tripDate\"), \"MM\")) \\\n","           .withColumn(\"day\", tripDate_format(col(\"tripDate\"), \"dd\"))\n","\n","dftripDate.show(truncate=False)\n","\n","dftripDate.write.partitionBy(\"year\", \"month\", \"day\").mode(\"overwrite\").parquet(commonPath + \"/partition/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d513bd6b-abd1-4061-acd8-2ec127d6b468","showTitle":false,"title":""},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Now, letâ€™s see how pruning works. \n","# For example,  the following query will only scan month=01 folder and skip all other folders.\n","readDF = spark.read.parquet(commonPath + \"/partition/year=2022\").filter(\"month=01\")\n","readDF.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0504c855-f269-4891-9b2f-c0d11d6d6259","showTitle":false,"title":""},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"DataPruningWithSpark-C2","notebookOrigID":188113580888522,"widgets":{}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"save_output":true,"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":0}
