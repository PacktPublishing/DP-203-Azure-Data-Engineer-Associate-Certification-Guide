{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f3b41bd4-0b99-4143-97d3-7b280145557d","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a5bfb54c-47df-4805-8476-2220852bae26","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eef106c4-e05f-4bd2-97db-b5d6830a95e2","showTitle":false,"title":""}},"outputs":[],"source":["%python\n","# Let us create a sample Dataframe with sensitive data\n","from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n","cols = StructType([ \\\n","    StructField(\"Name\",StringType(),True), \\\n","    StructField(\"SSN\",StringType(),True), \\\n","    StructField(\"email\",StringType(),True)\n","  ])\n"," \n","data = [(\"Adam Smith\",\"111-11-1111\",\"adam@adam.com\"),\n","    (\"Brenda Harman\",\"222-22-2222\",\"brenda@brenda.com\"),\n","    (\"Carmen Pinto\",\"333-33-3333\", \"carmen@carmen.com\")\n","  ]\n","\n","# Create the Dataframe\n","piidf = spark.createDataFrame(data=data,schema=cols)\n","display(piidf)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b218ae6b-1bcb-465a-b35e-f0bbacd54220","showTitle":false,"title":""}},"source":["To use the Fernet library, you will have to install encryption and crytography libraries. Go to the Compute tab of the Azure Databricks workspace. In the compute tab,  under Libraries  select PyPI and enter \"encryption\" in the textbox and click on \"Install new\" button. \n","Repeat the same process for the \"cryptography\" library also."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"262ff4a4-2e3e-4a90-8e7b-2d00116778fa","showTitle":false,"title":""}},"outputs":[],"source":["# Import the Fernet library and test it\n","from cryptography.fernet import Fernet\n","key = Fernet.generate_key()\n","f = Fernet(key)\n","token = f.encrypt(b\"Hello Azure\")\n","print(token)\n","print(f.decrypt(token))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"64327636-bb7f-4c64-9aaf-b34666195062","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# Define the encryption function\n","def encryptUdf(plaintext, KEY):\n","    from cryptography.fernet import Fernet\n","    f = Fernet(KEY)\n","    encryptedtext = f.encrypt(bytes(plaintext, 'utf-8'))\n","    return str(encryptedtext.decode('ascii'))\n","encrypt = udf(encryptUdf, StringType())\n","\n","\n","# decrypt udf\n","def decryptUdf(encryptedtext, KEY):\n","    from cryptography.fernet import Fernet\n","    f = Fernet(KEY)\n","    plaintext=f.decrypt( encryptedtext.encode()).decode()\n","    return plaintext\n","decrypt = udf(decryptUdf, StringType())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"57de160e-c84a-4582-bf2b-ca3d7fc7cc50","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import udf, lit, md5\n","from pyspark.sql.types import StringType\n","\n","# Fetch key from secrets or Azure Key vault\n","# encryptionKey = dbutils.preview.secret.get(scope = \"encrypt\", key = \"fernetkey\")\n","encryptionKey = key\n","# Encrypt the data \n","encrypteddf = piidf.withColumn(\"SSN\", encrypt(\"SSN\", lit(encryptionKey)))\n","display(encrypteddf)\n","\n","#Save encrypted data \n","encrypteddf.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"PIIEncryptedTable\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ef33ea99-3778-49ba-bd33-a24a98e1b657","showTitle":false,"title":""}},"outputs":[],"source":["decrypted = encrypteddf.withColumn(\"SSN\", decrypt(\"SSN\",lit(encryptionKey)))\n","display(decrypted)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"093d4b51-2fda-4bb8-a453-69375e896139","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"HandlingSensitiveInfoInDataframe-C12","notebookOrigID":188113580888575,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
