{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2036131c-6bed-4364-81ef-e0ee7383e79b","showTitle":false,"title":""},"microsoft":{}},"outputs":[],"source":["%scala\n","// RDD Transformation examples\n","\n","// Let us take a sample list of cities and perform transformations on it.\n","val cities = Seq(\"New York\",\n","  \"New Jersey\",\n","  \"San Francisco\",\n","  \"Phoenix\",\n","  \"Seattle\",\n","  \"Austin\",\n","  \"Atlanta\",\n","  \"Miami\",\n","  \"Salt Lake City\",\n","  \"Tempe\",\n","  \"San Jose\",\n","  \"Chicago\",\n","  \"San Jose\",\n","  \"Miami\",\n","  \" \",\n","  \"Austin\")\n","\n","// Creat the RDD\n","val rdd=spark.sparkContext.parallelize(cities)\n","\n","// Map transformation\n","val maprdd=rdd.map( f => (f,1))\n","maprdd.collect.foreach(println)\n","\n","//FlatMap transformation\n","val fmrdd = rdd.flatMap(word => word.split(\" \"))\n","fmrdd.collect.foreach(println)\n","\n","//Filter transformation\n","val filterrdd = rdd.filter(word => word.contains(\" \"))\n","filterrdd.collect.foreach(println)\n","\n","// Filtering out empty entries\n","val emptystrfilterrdd = rdd.filter(_.nonEmpty)\n","emptystrfilterrdd.collect.foreach(println)\n","\n","// groupby transformation\n","val groupbyrdd = rdd.groupBy(word => word.charAt(0))\n","groupbyrdd.collect.foreach(println)\n","\n","//Union transformation\n","val rdd1 = spark.sparkContext.parallelize(List(1, 2, 3))\n","val rdd2 = spark.sparkContext.parallelize(List(4, 5, 6))\n","\n","val unionrdd = rdd1.union(rdd2)\n","unionrdd.collect().foreach(println)\n","\n","//Distinct transformation\n","val distrinctrdd = rdd.distinct()\n","distrinctrdd.collect.foreach(println)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ad7f0c3f-148c-4cb0-9129-ce501dffd521","showTitle":false,"title":""},"collapsed":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%scala\n","// Dataframe Transformation examples\n","\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","// Let us define a sample Dataframe\n","val driverDetails = Seq(\n","    Row(\"Alice\",\"\",\"Hood\",\"100\",\"New York\", \"Female\", 4100),\n","    Row(\"Bryan\",\"M\",\"Williams\",\"101\",\"New York\",\"Male\", 4000),\n","    Row(\"Catherine\",\"Goodwin\",\"\",\"102\",\"California\",\"Female\", 4300),\n","    Row(\"Daryl\",\"\",\"Jones\",\"103\",\"Florida\",\"Male\", 5500),\n","    Row(\"Jenny\",\"Anne\",\"Simons\",\"104\",\"Arizona\",\"Female\", 3400),\n","    Row(\"Daryl\",\"\",\"Jones\",\"103\",\"Florida\",\"Male\", 5500)\n","  )\n","\n","// Define the schema\n","val driverSchema = new StructType().add(\"firstName\", StringType).add(\"middleName\", StringType).add(\"lastName\",StringType).add(\"id\",StringType).add(\"location\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n","\n","// Create the Dataframe\n","val driverDf = spark.createDataFrame(\n","    spark.sparkContext.parallelize(driverDetails),driverSchema)\n","  driverDf.printSchema()\n","  driverDf.show(false)\n","\n","// select transformation\n","driverDf.select(\"firstname\",\"lastname\").show()\n","\n","// filter tranformation\n","driverDf.filter('location === \"Florida\").show(false)\n","\n","// distinct transformation\n","driverDf.distinct().show(false)\n","\n","// Sortby\n","driverDf.sort(\"lastname\",\"firstname\").show(false)\n","\n","// Orderby\n","driverDf.orderBy(\"location\").show(false)\n","\n","//groupby transformation\n","driverDf.groupBy(\"location\").avg(\"salary\").show(false)\n","\n","// Join\n","// For the join, let us create one more datafram called driverRating\n","\n","val driverRating = Seq(\n","    Row(\"100\", 5),\n","    Row(\"101\", 4),\n","    Row(\"102\", 3),\n","    Row(\"103\", 5),\n","    Row(\"104\", 2),\n","    Row(\"103\",4)\n","  )\n","\n","// Define the schema\n","val ratingSchema = new StructType().add(\"id\",StringType).add(\"rating\",IntegerType)\n","\n","// Create the Dataframe\n","val ratingDf = spark.createDataFrame(\n","    spark.sparkContext.parallelize(driverRating),ratingSchema)\n","  ratingDf.printSchema()\n","  ratingDf.show(false)\n","\n","driverDf.join(ratingDf, driverDf(\"id\") ===  ratingDf(\"id\"),\"inner\").show(false)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6aa1fe5c-45a4-4064-b519-814281a44c09","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkTransformations-C8","notebookOrigID":188113580888554,"widgets":{}},"description":null,"kernelspec":{"display_name":"Synapse Spark","name":"synapse_spark"},"language_info":{"name":"scala"},"save_output":false},"nbformat":4,"nbformat_minor":0}
